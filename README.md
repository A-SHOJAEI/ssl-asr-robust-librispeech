# ssl-asr-robust-librispeech

Robust ASR under additive noise and room reverberation, using a self-supervised (wav2vec2-style) pretraining stage and a consistency-regularized CTC fine-tuning stage. The repository is wired end-to-end to produce:

- machine-readable metrics: `artifacts/results.json`
- a human report with the exact same numbers: `artifacts/report.md`

## Problem Statement

Train an ASR model that is resilient to distribution shift from clean speech to noisy and reverberant speech, using only:

- clean, labeled speech for supervised ASR training / fine-tuning
- unlabeled speech for SSL pretraining
- noise (MUSAN) and impulse responses (RIRS_NOISES) for augmentation and robust evaluation

## Dataset Provenance (What This Repo Downloads/Uses)

This repository does not ship audio; it generates JSONL manifests that point to local files.

- LibriSpeech (OpenSLR SLR12): speech + transcripts used for supervised training and for SSL pretraining audio.
- MUSAN (OpenSLR SLR17): additive noise used during fine-tuning (`src/ssl_asr/data/augment.py`) and for deterministic robust eval manifests (`scripts/make_noisy_eval.py`).
- RIRS_NOISES (OpenSLR SLR28): room impulse responses used similarly for reverb augmentation and robust eval.
- Optional 4-gram LM (OpenSLR SLR11): download supported by `scripts/download_openslr.py`, but decoding in this repo is greedy CTC by default (no LM).

Downloader details are implemented in `scripts/download_openslr.py` (resume support, safe extraction helpers, and a `data/raw/download_manifest.json` with checksums/paths).

## Methodology (Implemented)

### Baseline: Supervised Conformer-CTC

- Model: `src/ssl_asr/models/conformer_ctc.py` (log-mel frontend via `torchaudio`, Conformer blocks, CTC head).
- Training: `src/ssl_asr/train_asr_baseline.py` optimizes CTC loss (`src/ssl_asr/models/ctc.py`).
- Optional SpecAugment is configurable via `baseline.{time_mask_param,freq_mask_param,num_*_masks}` in YAML configs.

### SSL Pretraining: Simplified wav2vec2-Style Contrastive Objective

- Model: `src/ssl_asr/models/wav2vec2.py` (conv feature encoder + TransformerEncoder context).
- Objective: InfoNCE on randomly selected time steps via `info_nce_loss(...)`.
  - Note: this implementation does not include product quantization, and it selects masked positions for the loss but does not apply a learnable mask embedding to the input features.
- Training entrypoint: `src/ssl_asr/pretrain_ssl.py`.

### Fine-Tuning: wav2vec2-CTC + Consistency Regularization + On-the-Fly Augment

- Model: `Wav2Vec2Model` + linear CTC head (`Wav2Vec2CTC`).
- Initialization: `--init_from` loads only the SSL `Wav2Vec2Model` weights into the CTC model base (`src/ssl_asr/finetune_asr.py`).
- Augmentations (two independent views per sample):
  - Additive noise at random SNR: MUSAN if configured, else Gaussian noise fallback (smoke).
  - Reverberation: FFT convolution with a randomly sampled RIR file.
  - Implemented in `src/ssl_asr/data/augment.py` and invoked from `src/ssl_asr/finetune_asr.py`.
- Loss:
  - supervised CTC on one view
  - + `consistency_weight * symmetric_kl(logp(view1), logp(view2))`

## Baselines and Ablations (Implemented Switches)

These are real flags/config knobs in the codebase:

- Baseline model: `ssl_asr.train_asr_baseline` (Conformer-CTC).
- SSL + fine-tune: `ssl_asr.pretrain_ssl` then `ssl_asr.finetune_asr --init_from runs/<ssl>/checkpoints/last.pt`.
- No-consistency ablation: `ssl_asr.finetune_asr --consistency_weight 0.0`.
- No-SSL ablation: omit `--init_from` (randomly initialized wav2vec2 backbone).
- No augmentation: `--augment_musan false --augment_rir false` (augmentation code paths are in `src/ssl_asr/data/augment.py`).

## Results (Exact Numbers in This Repo)

`artifacts/report.md` is generated from `artifacts/results.json` by `scripts/make_report.py`.

The current committed artifacts come from the default smoke pipeline (`configs/smoke.yaml`), which trains for 30 steps on a tiny synthetic dataset generated by `scripts/make_smoke_data.py`. WER values are therefore not research-meaningful. Generated on 2026-02-20.

Reference: `artifacts/report.md` -> `## Summary` -> `### Set: test`:

| Experiment | Checkpoint | WER | 95% CI | #utts | #ref words |
|---|---|---:|---:|---:|---:|
| baseline | `runs/smoke_baseline/checkpoints/last.pt` | 1.0000 | [1.0000, 1.0000] | 8 | 17 |
| finetune | `runs/smoke_finetune/checkpoints/last.pt` | 1.0000 | [1.0000, 1.0000] | 8 | 17 |
| ablation_no_consistency | `runs/smoke_ablation_no_consistency/checkpoints/last.pt` | 1.0000 | [1.0000, 1.0000] | 8 | 17 |

No figures are currently generated; the report is a Markdown table plus notes.

Metric details:

- Decoding: greedy CTC (`src/ssl_asr/models/ctc.py`), no language model.
- WER: `jiwer` via `src/ssl_asr/eval/wer.py` after normalization in `src/ssl_asr/data/text.py`.
- 95% CI: bootstrap over per-utterance WER (500 resamples, seed=0) in `src/ssl_asr/eval/bootstrap.py`.

## Reproducibility

Dependencies are pinned in `requirements.txt` (notably `torch==2.5.1`, `torchaudio==2.5.1`).

### 1) Smoke (Offline, Fast)

```bash
make all
```

What you get:

- `runs/` (per-run `config.json`, `env.json`, `train_metrics.json`, checkpoints)
- `artifacts/results.json` (from `ssl_asr.evaluate_asr`)
- `artifacts/report.md` (from `scripts/make_report.py`)

### 2) Real LibriSpeech + MUSAN + RIRS (Downloads Required)

1. Create env:

```bash
make setup
```

2. Download and extract OpenSLR corpora (example: full 960h + MUSAN + RIRS + optional LM):

```bash
.venv/bin/python scripts/download_openslr.py \
  --out data/raw \
  --librispeech ls960 \
  --musan --rirs --lm 4gram \
  --extract
```

3. Build manifests (JSONL pointing to extracted audio paths):

```bash
.venv/bin/python scripts/prepare_librispeech.py \
  --in data/raw \
  --out data/manifests \
  --subsets train-clean-100,dev-clean,dev-other,test-clean,test-other
```

4. (Optional) Create deterministic robust eval manifests (noise-only, RIR-only, and combined):

```bash
.venv/bin/python scripts/make_noisy_eval.py \
  --manifests data/manifests \
  --musan data/raw/musan \
  --rirs data/raw/RIRS_NOISES \
  --snrs 0,5,10,20 \
  --out data/eval_robust
```

5. Point a config at your manifests (start from `configs/full_example.yaml`), then:

```bash
make train CONFIG=configs/full_example.yaml
make eval  CONFIG=configs/full_example.yaml
make report
```

Tip: `src/ssl_asr/data/manifests.py` expects your audio to already be at the target sample rate (typically 16 kHz); it will raise on mismatches (no resampling).

## Limitations (Current State)

- The committed metrics are from `data/smoke` (synthetic audio) and should not be used as evidence of robustness.
- SSL pretraining is intentionally simplified (no quantization; no feature masking with a mask embedding), so it is not a faithful reproduction of wav2vec2 2.0.
- Decoding is greedy CTC only; no beam search, no lexicon/LM integration.
- Augmentation sampling is file-uniform (random file from `musan_dir` / `rirs_dir`), with no category balancing (e.g., speech/music/noise in MUSAN).
- `JsonlSpeechDataset` enforces sample-rate equality rather than resampling, which can be inconvenient for mixed-SR corpora.

## Next Research Steps (Concrete, Repo-Aligned)

1. Add evaluation sets in `configs/full_example.yaml` (`eval_sets:`) for `test-other` and the `data/eval_robust/*` manifests, then report WER deltas across SNR/RIR conditions.
2. Make SSL pretraining closer to wav2vec2 by adding a learnable mask embedding and masking the feature sequence before the Transformer context network.
3. Add an LM decoding path (e.g., `pyctcdecode` + optional KenLM) behind a flag in `ssl_asr.evaluate_asr`, and report greedy vs beam/LM gaps.
4. Expand ablations to isolate each factor: (a) MUSAN only, (b) RIR only, (c) consistency only, (d) SSL only, and report interaction effects.
